---
title: "Modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, warning=FALSE, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(GGally)
```

### Load data

```{r load-data, include=FALSE}
load("movies.Rdata")
```


* * *

## Part 1: Data

As shown in the codebook, the data set is comprised of 651 randomly sampled movies produced and released before 2016. Hence, the inference method only implicates generalization rather than causality.

* * *

## Part 2: Research question

What attributes do make a movie popular in Rotten Tomatoes?

* * *

## Part 3: Exploratory data analysis

- Predictors:
    - "title_type"
    - "genre"
    - "runtime"
    - "mpaa_rating"
    - "thtr_rel_month"
    - "critics_rating"
    - "critics_score"
    - "best_pic_nom"
    - "best_pic_win"
    - "best_actor_win"
    - "best_actress_win"
    - "best_dir_win"
    - "top200_box"

```{r, warning=FALSE, message = FALSE}
# subset the needed columns
predictors <- movies[, c(2,3,4,5,8,15,16,19,20,21,22,23,24)]
str(predictors)
```

- Check best_*_wins and top200_box

```{r, warning=FALSE, message = FALSE}
# Intuitively, more bests mean higher ratings, so I sum up the binary counts of "best_pic_nom", "best_pic_win", "best_actor_win", "best_actress_win", "best_dir_win", "top200_box"
predictors$bests <- rowSums(sapply(predictors[,8:13], function(x) ifelse(x == 'yes', 1, 0)))
# remove binary counts
predictors <- predictors[,-(8:13)]
```

- Check the relationship between critics ratings and scores

```{r, warning=FALSE, message = FALSE}
ggpairs(predictors, columns = c(6,7))
# The critics score plotting are nearly bimodel whose peaks accord with the ratings of 'Rotten' and 'Certified Fresh'. The middle level of Fresh is quite close to Certified Fresh, which makes the distribution a bimodel rather than a trimodel. Here I only keep the numeric score and drop the categorical ratings.

# remove critic ratings
predictors$critics_rating <- NULL
```

- Check the relationship between title type and genre

```{r, warning=FALSE, message = FALSE}
ggpairs(predictors, columns = c(1,2))
# The feature film dominates the title type, I tend to drop tile_type since it doesn't contribute much variance.

# remove title_type
predictors$title_type <- NULL
```

- Check correlations between numeric variables

```{r, warning=FALSE, message = FALSE}
ggpairs(predictors, columns = c('runtime', 'thtr_rel_month', 'critics_score', 'bests'))
# Not much high corrections, so we are going to keep them all
```

- Labels:

    - "audience score"

```{r, warning=FALSE, message = FALSE}
# Labels: since this is a regression model, so the numeric audience score is used as the label rather than the categorical variabel audience rating
label <- movies[, c(18)]
str(label)
# Pair-plooting the audience score and rating
ggpairs(movies, columns = c(17,18))
# The audience score plotting are bimodel whose peaks basically accord with the binary audience rating of 'Spilled' and 'Upright'. It makes sense.
```

- Combine predictors and label

```{r, warning=FALSE, message = FALSE}
movies.rt <- cbind(predictors, label)
str(movies.rt)
```

- sample the data for train and test
```{r, warning=FALSE, message = FALSE}
# sample the data for train and test
set.seed(100)
len <- nrow(movies)
idx <- sample(len, size=0.9 * len, replace=FALSE)
# test data
movies.test <- movies.rt[-idx, ]

# train data
movies.rt <- movies.rt[idx, ]
```

* * *

## Part 4: Modeling

```{r, warning=FALSE, message = FALSE}
m.rt <- lm(audience_score ~ ., data = movies.rt)
summary(m.rt)
# Adjusted R-squared:  0.5208
```

- Backward model selection

First drop: from the summary, thtr_rel_month, bests, and mpaa_rating are not significant. Let's drop them alternatively to pick the model that increases most.

```{r, warning=FALSE, message = FALSE}
# remove month
m.rt.1.month <- lm(audience_score ~ genre + runtime + mpaa_rating + critics_score + bests, data = movies.rt)
summary(m.rt.1.month)
# Adjusted R-squared:  0.5214

# remove mpaa_rating
m.rt.1.mpaa_rating <-lm(audience_score ~ genre + runtime + thtr_rel_month + critics_score + bests, data = movies.rt)
summary(m.rt.1.mpaa_rating)
# Adjusted R-squared:  0.5236

# remove bests
m.rt.1.bests <- lm(audience_score ~ genre + runtime + mpaa_rating + thtr_rel_month + critics_score, data = movies.rt)
summary(m.rt.1.bests)
# Adjusted R-squared:  0.5217
```

Pick the model of m.rt.1.mpaa_rating that remove mpaa_rating with highest increase of adjusted R2 from 0.5208 to 0.5236

Second drop: based on the previous drop, drop bests and thtr_rel_month

```{r, warning=FALSE, message = FALSE}
# remove bests
m.rt.2.bests <- lm(audience_score ~ genre + runtime + thtr_rel_month + critics_score, data = movies.rt)
summary(m.rt.2.bests)
# Adjusted R-squared: 0.5245

# remove thtr_rel_month
m.rt.2.month <- lm(audience_score ~ genre + runtime + critics_score + bests, data = movies.rt)
summary(m.rt.2.month)
# Adjusted R-squared: 0.5243
```

Droping thtr_rel_month have the same increase of adjusted R2 from 0.5236 to 0.5245

Third drop: drop both bests

```{r, warning=FALSE, message = FALSE}
m.rt.3 <- lm(audience_score ~ genre + runtime + critics_score, data = movies.rt)
summary(m.rt.3)
# Adjusted R-squared: 0.5251
```

Droping both bests and thtr_rel_month have the same increase of adjusted R2 from 0.5245 to 0.5251

Forth drop: drop runtime, critics_score, and genre alternatively to check the R2 increase

```{r, warning=FALSE, message = FALSE}
m.rt.4.runtime <- lm(audience_score ~ genre + critics_score, data = movies.rt)
summary(m.rt.4.runtime)
# Adjusted R-squared:  0.5207

m.rt.4.genre <- lm(audience_score ~ runtime + critics_score, data = movies.rt)
summary(m.rt.4.genre)
# Adjusted R-squared:  0.4959

m.rt.4.critics <- lm(audience_score ~ genre + runtime, data = movies.rt)
summary(m.rt.4.critics)
# Adjusted R-squared: 0.2213
```

Fourth drop doesn't show any increase in R2, so our model selection stop here. m.rt.3 is our final model.

- Diagnostics for MLR

Use residual plots to evaluate whether the conditions of least squares regression are reasonable.

```{r, warning=FALSE, message = FALSE}
ggplot(m.rt.3, aes(.fitted, .resid)) + 
    geom_jitter()
```

- Linear association: The residuals plot doesn't show a random scatter.
- Constant variance of residuals: residuals vary less in the right than left.
- Nearly normal residuals: Residuals are slightly right skewed
- Independent observations: Classes sampled randomly

Conclusion: this model doesn't quite meet the requirements for MLR.

* * *

## Part 5: Prediction

```{r, warning=FALSE, message = FALSE}
movies.test <- movies.test[, c('genre','runtime','critics_score', 'audience_score')]

# predict with CI of 95%
preds.95 <- predict(m.rt.3, movies.test[, -c(4)], interval = "prediction", level = 0.95)

preds.95 <- cbind(preds.95, movies.test$audience_score)
colnames(preds.95)[4] <- "score"
preds.95 <- as.data.frame(preds.95)

# Evalutate 95% CI
preds.95$cover <- mutate(preds.95, cover = ifelse(score >= lwr & score <= upr, TRUE, FALSE))
# sum the predictions that CI covers real audience score
sum(preds.95$cover == TRUE) / nrow(preds.95)
```


* * *

## Part 6: Conclusion

Based on the fitted model m.rt.3, critics_score, runtime, and genere contribute most to the model. critics_score has the highest weight of 0.44348. In Genre, different genres have different effect on the score. For example, Documentary and Musical & Performing Arts have positive effect, while Science Fiction & Fantasy and Horror tend to have negative effect. Runtime has a little effect on the socre.

This model doesn't provide a high R2(slightly higher 50%), which is also shown in the Diagnostics result. A polynomial model is worth trying for regession approach. Additionally, there are many categorical features in the dataset, it's better to choose other methods like decision tree to imporve the prediction accuracy. But it's beyond the scope of this class.

